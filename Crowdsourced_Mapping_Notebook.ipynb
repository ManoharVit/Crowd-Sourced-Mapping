{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Crowdsourced Mapping** - (Mulivariate Classification)"
      ],
      "metadata": {
        "id": "mt3BGqu2QWwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dulr6dcFQGcX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Activation\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "import timeit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset:-"
      ],
      "metadata": {
        "id": "ryg49-IfQcwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('training.csv')\n",
        "test_data = pd.read_csv('testing.csv')"
      ],
      "metadata": {
        "id": "tN-TVsMEQcQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Exploration"
      ],
      "metadata": {
        "id": "hfO-pZj8QnnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()"
      ],
      "metadata": {
        "id": "s0Tx9sT3Qkwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.describe(include=\"all\")"
      ],
      "metadata": {
        "id": "LoATjxfJQwku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unitr = train_data['class'].unique()\n",
        "print(f\"Total number of unique in class '\",len(unitr), \"' and they are \\n\", unitr)"
      ],
      "metadata": {
        "id": "n9WaCQLwQ3Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean for each class\n",
        "class_means = train_data.groupby('class').mean()\n",
        "\n",
        "# Display the mean values for each class\n",
        "class_means"
      ],
      "metadata": {
        "id": "s88QCUL-RJG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot line plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "for class_label, values in class_means.iterrows():\n",
        "    plt.plot(class_means.columns, values, label=class_label)\n",
        "\n",
        "plt.title('Distribution of Max_ndvi Over Time for Different Classes')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Max_Ndvi Values')\n",
        "plt.legend(title='Class Lables', bbox_to_anchor=(1.05, 1), loc='upper right')\n",
        "\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q8b8TTYHR2P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_class = class_means\n",
        "\n",
        "# Plot the distribution of each row\n",
        "plt.figure(figsize=(13, 6))\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "for index, row in df_no_class.iterrows():\n",
        "    sns.kdeplot(row, label=index, fill=True)\n",
        "\n",
        "plt.title('Distribution of Each Row')\n",
        "plt.xlabel('Feature Values')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "StGGJW9ER6ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(class_means, index=['farm', 'forest', 'grass', 'impervious', 'orchard', 'water'])\n",
        "df = df.drop('max_ndvi', axis=1)\n",
        "\n",
        "# Plot the distribution of each row\n",
        "for index, row in df.iterrows():\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.bar(row.index, row.values, color='skyblue')\n",
        "    plt.title(f'Distribution for {index}')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Value')\n",
        "    plt.xticks(rotation=90, ha=\"right\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1WRKuMB4SD0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainx, y_train = train_data.iloc[:, 1:], train_data.iloc[:, 0]\n",
        "x_test, y_test = test_data.iloc[:, 1:], test_data.iloc[:, 0]"
      ],
      "metadata": {
        "id": "5Zzaryx5cy4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying min-max scaling"
      ],
      "metadata": {
        "id": "I1EWoxxJSVO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform min-max scaling\n",
        "def min_max_scaling(data):\n",
        "    min_val = np.min(data, axis=0)\n",
        "    max_val = np.max(data, axis=0)\n",
        "    scaled_data = (data - min_val) / (max_val - min_val)\n",
        "    return scaled_data\n",
        "\n",
        "# Fit and transform trainx\n",
        "min_val_train = np.min(trainx, axis=0)\n",
        "max_val_train = np.max(trainx, axis=0)\n",
        "trainx = (trainx - min_val_train) / (max_val_train - min_val_train)\n",
        "\n",
        "# Transform x_test using the min and max values from trainx\n",
        "x_test = (x_test - min_val_train) / (max_val_train - min_val_train)"
      ],
      "metadata": {
        "id": "F0Sh-xsBSUNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution of different classes with in the training dataset:-"
      ],
      "metadata": {
        "id": "kt-mKpidSrsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To understand the distribution among the different classes in training data.\n",
        "train_data['class'].value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "DsTjf4VHS2oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['class'].value_counts()"
      ],
      "metadata": {
        "id": "Ex8eBLubTSia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regarding the distribution, it's evident that nearly 75% of the data points are attributed to a single class, highlighting a pronounced imbalance in the dataset. Some classes have only 205 and 53 instances, respectively, out of the total 10,545."
      ],
      "metadata": {
        "id": "Fu6cNZMETU1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To tackle the data imbalance challenge, we implemented SMOTE oversampling, resulting in an expanded dataset of nearly 45,000 data pointsâ€”three times the size of the original set. Instead of using a widespread oversampling method that could overshadow the original data, we chose to focus on boosting the representation of minority classes and also preserve the original dataset(means:- oversampling data should not dominate the original data). This specific oversampling approach contributed to better generalization and enhanced prediction capabilities."
      ],
      "metadata": {
        "id": "q2HNRDf5Uff1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_column = 'class'\n",
        "X = trainx\n",
        "y = train_data['class']\n",
        "\n",
        "# Oversample the minority class\n",
        "oversample = SMOTE(sampling_strategy={'water': 2000, 'farm': 2000, 'orchard': 2000, 'grass': 2000, 'impervious': 2000})\n",
        "\n",
        "# Undersample the majority class\n",
        "undersample = RandomUnderSampler(sampling_strategy={'forest': 7000})\n",
        "\n",
        "# Create a pipeline that first oversamples, then undersamples\n",
        "pipeline = Pipeline([('o', oversample), ('u', undersample)])\n",
        "\n",
        "# Apply the pipeline to your data\n",
        "X_resampled, y_resampled = pipeline.fit_resample(X, y)\n",
        "\n",
        "# The resampled feature set and target\n",
        "resampled_data = pd.DataFrame(X_resampled, columns=train_data.drop(target_column, axis=1).columns)\n",
        "resampled_data['class'] = y_resampled\n",
        "\n",
        "# Now, 'resampled_data' is the balanced dataset\n",
        "resampled_data"
      ],
      "metadata": {
        "id": "_HLEhndIUcZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_data['class'].value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "72EV36SxYrJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation analysis:-"
      ],
      "metadata": {
        "id": "shDra3_9ZKmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = X_resampled.corr()\n",
        "\n",
        "# Heatmap using seaborn\n",
        "plt.figure(figsize=(20,20))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.25)\n",
        "plt.title('Plot for Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DGhH5lvlY6MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apart from one specific pair of columns in the dataset, we did not identify any other significant correlations throughout the entire dataset.\n"
      ],
      "metadata": {
        "id": "yHtOrUnjPEzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dimensionality Reduction"
      ],
      "metadata": {
        "id": "ET2KGbiAZnFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We are planning to deploy two dimensionality reduction techniques:\n",
        "\n",
        "a.   Principal Component Analysis (PCA)\n",
        "\n",
        "b.   t-Distributed Stochastic Neighbor Embedding (t-SNE)"
      ],
      "metadata": {
        "id": "SUwseWrdaFEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis"
      ],
      "metadata": {
        "id": "ZcPdhjDu3mmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PCA:\n",
        "\n",
        "    def __init__(self, desired_components=None):\n",
        "        self.desired_components = desired_components\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "\n",
        "        self.mean_ = np.mean(X, axis=0)\n",
        "        X_centered = X - self.mean_\n",
        "        cov_matrix = np.cov(X_centered.T)\n",
        "\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "        eigen_indices = np.argsort(eigenvalues)[::-1]  # Sort in descending order\n",
        "\n",
        "        self.eigenvalues_ = eigenvalues[eigen_indices]\n",
        "        self.eigenvectors_ = eigenvectors[:, eigen_indices]\n",
        "\n",
        "        if self.desired_components is not None:\n",
        "            self.eigenvectors_ = self.eigenvectors_[:, :self.desired_components]\n",
        "\n",
        "        X_transformed = np.dot(X_centered, self.eigenvectors_)\n",
        "\n",
        "        return X_transformed\n",
        "\n",
        "    def transform(self, X):\n",
        "\n",
        "        # Transforms data to the principal component space.\n",
        "\n",
        "        # Check if already fit\n",
        "\n",
        "        if not hasattr(self, \"mean_\"):\n",
        "            raise ValueError(\"Model not fitted yet! Call `fit` or `fit_transform` first.\")\n",
        "\n",
        "        X_centered = X - self.mean_\n",
        "\n",
        "        return np.dot(X_centered, self.eigenvectors_)\n",
        "\n",
        "    def inverse_transform(self, X):\n",
        "        # Transforms data back to the original space.\n",
        "\n",
        "        # Check if already fit\n",
        "\n",
        "        if not hasattr(self, \"mean_\"):\n",
        "            raise ValueError(\"Model not fitted yet! Call `fit` or `fit_transform` first.\")\n",
        "\n",
        "        return np.dot(X, self.eigenvectors_.T) + self.mean_\n",
        "\n",
        "    def explained_variance_ratio_(self):\n",
        "\n",
        "        if not hasattr(self, \"eigenvalues_\"):\n",
        "            raise ValueError(\"Model not fitted yet! Call `fit` or `fit_transform` first.\")\n",
        "\n",
        "        return self.eigenvalues_ / np.sum(self.eigenvalues_)\n",
        "\n",
        "    def variance_captured(self):\n",
        "\n",
        "        if not hasattr(self, \"desired_components\"):\n",
        "            raise ValueError(\"Number of components not specified.\")\n",
        "\n",
        "        if not hasattr(self, \"eigenvalues_\"):\n",
        "            raise ValueError(\"Model not fitted yet! Call `fit` or `fit_transform` first.\")\n",
        "\n",
        "        captured_variance = np.sum(self.eigenvalues_[: self.desired_components]) / np.sum(self.eigenvalues_)\n",
        "        return captured_variance"
      ],
      "metadata": {
        "id": "jIz-OkH-aBeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_resampled\n",
        "desired_components=28\n",
        "# Performing PCA\n",
        "pca = PCA(desired_components)\n",
        "pca_results = pca.fit_transform(features)\n",
        "\n",
        "print(\"The Variance Captured by\", desired_components, \"components:\", round(pca.variance_captured(), 2) * 100, \"%\", '\\n')\n"
      ],
      "metadata": {
        "id": "7J9NVfZ9bVuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " t - Distributed Stochastic Neighbor Embedding"
      ],
      "metadata": {
        "id": "invmNLVL12hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Labels = resampled_data['class']\n",
        "\n",
        "# For plotting, we create a color map\n",
        "unique_labels = np.unique(Labels)\n",
        "colours = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
        "color_map = dict(zip(unique_labels, colours))\n",
        "\n",
        "# Executing t-SNE on curated data\n",
        "features = X_resampled\n",
        "tsne = TSNE(n_components=3, perplexity=150, verbose=1, n_iter=500)\n",
        "tsne_results = tsne.fit_transform(features)"
      ],
      "metadata": {
        "id": "95ffIk0FA_O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne_results.shape"
      ],
      "metadata": {
        "id": "tzB-5lZMzyyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Labels = y_resampled\n",
        "df = pd.DataFrame({'Y1': tsne_results[:, 0], 'Y2': tsne_results[:, 1],'Y3': tsne_results[:, 2], 'label': Labels})\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(data=df, x='Y1', y='Y2', hue='label')\n",
        "\n",
        "# Setting plot title and labels\n",
        "plt.title('t-SNE Visualization')\n",
        "plt.xlabel('Y1')\n",
        "plt.ylabel('Y2')\n"
      ],
      "metadata": {
        "id": "xck1cb45TOwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We opt for incorporating elevated perplexity values as an alternative to expanding the iteration count, aiming to streamline the execution time."
      ],
      "metadata": {
        "id": "gaO6uHcKI10I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the results to a DataFrame\n",
        "df_tsne = pd.DataFrame(tsne_results, columns=['Y1', 'Y2','Y3'])\n",
        "\n",
        "# Display the first few rows\n",
        "df_head = df_tsne.head()\n",
        "df_head"
      ],
      "metadata": {
        "id": "Qu3pVAVpTkFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "    'Y1': tsne_results[:, 0],\n",
        "    'Y2': tsne_results[:, 1],\n",
        "    'Y3': tsne_results[:, 2],\n",
        "    'label': Labels\n",
        "})\n",
        "\n",
        "# For plotting, create a color palette\n",
        "palette = sns.color_palette(\"hsv\", len(df['label'].unique()))\n",
        "# Create a 3D scatter plot using plotly\n",
        "fig = px.scatter_3d(df, x='Y1', y='Y2', z='Y3', color='label',\n",
        "                    color_discrete_sequence=['#D53E4F', '#FC8D59', '#FEE08B', '#FFFFBF', '#E6F598', '#99D594'])\n",
        "\n",
        "# Update layout for axes titles\n",
        "fig.update_layout(scene=dict(\n",
        "                    xaxis_title='Y1',\n",
        "                    yaxis_title='Y2',\n",
        "                    zaxis_title='Y3'))\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "e5eQUkPnUX_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're using something called t-SNE to simplify complex patterns in data. But, unfortunately, it's tough to clearly see the different groups in the data. We've been tweaking some hyperparameters to improve this, but there are still a lot of areas where the groups overlap."
      ],
      "metadata": {
        "id": "cThnHL8FJjv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Models and Performance evaluation"
      ],
      "metadata": {
        "id": "85nSP8YtLvoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification models used for the project:\n",
        "\n",
        "1. **Logistic Regression:**\n",
        "   \n",
        "   Simple and efficient for binary classification.\n",
        "   \n",
        "   Assumes a linear relationship between the features and the log-odds of the response.\n",
        "\n",
        "4. **Neural Networks:**\n",
        "\n",
        "   Deep learning models with multiple layers of neurons.\n",
        "   \n",
        "   Powerful for complex tasks but may require a large amount of data."
      ],
      "metadata": {
        "id": "9SututJW9DOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression"
      ],
      "metadata": {
        "id": "CGsc-69YdLOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the LabelEncoder object\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fitting the encoder to the training data and transforming both train and test labels\n",
        "#y_train_encoded = encoder.fit_transform(y_resampled)\n",
        "y_test_encoded = encoder.fit_transform(y_test)\n",
        "y_train_encoded = encoder.fit_transform(y_train)"
      ],
      "metadata": {
        "id": "BQu6ielP5hDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "class LogisticRegression:\n",
        "    def __init__(self, X_train, y_train, X_test, y_test, learningRate, tolerance, maxIteration = 1000, indexes=[]):\n",
        "        self.learningRate = learningRate\n",
        "        self.tolerance = tolerance\n",
        "        self.maxIteration = maxIteration\n",
        "        self.indexes = indexes\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "    '''\n",
        "    def datasetReader(self, indexes):\n",
        "      X_train = x_train\n",
        "      y_train = y_train_encoded\n",
        "      X_test = x_test\n",
        "      y_test = y_test_encoded\n",
        "      return X_train, y_train, X_test, y_test\n",
        "    '''\n",
        "\n",
        "    def addX0(self, X):\n",
        "        return np.column_stack([np.ones([X.shape[0], 1]), X])\n",
        "\n",
        "    def sigmoid(self, z_value):\n",
        "        sig_value = 1/(1 + np.exp(-z_value))\n",
        "        return sig_value\n",
        "\n",
        "    def costFunction(self, X, y):\n",
        "        pred_value_ = np.log(np.ones(X.shape[0]) + np.exp(X.dot(self.w))) - X.dot(self.w)*y\n",
        "        cost_value = pred_value_.sum()\n",
        "        return cost_value\n",
        "\n",
        "    def gradient(self, X, y):\n",
        "        sig_value = self.sigmoid(X.dot(self.w))\n",
        "        gradient_value = (sig_value - y).dot(X)\n",
        "        return gradient_value\n",
        "\n",
        "    def gradientDescent(self, X, y):\n",
        "        cost_sequences = []\n",
        "        last_cost = float('inf')\n",
        "        for i in tqdm(range(self.maxIteration)):\n",
        "            self.w = self.w - self.learningRate * self.gradient(X, y)\n",
        "            cur_cost = self.costFunction(X, y)\n",
        "\n",
        "            diff = last_cost - cur_cost\n",
        "            last_cost = cur_cost\n",
        "            cost_sequences.append(cur_cost)\n",
        "            if diff < self.tolerance:\n",
        "                print('The model Converged')\n",
        "                break\n",
        "\n",
        "        self.plotCost(cost_sequences)\n",
        "        return\n",
        "\n",
        "    def plotCost(self, error_sequences):\n",
        "        s = np.array(error_sequences)\n",
        "        t = np.arange(s.size)\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.plot(t, s)\n",
        "        ax.set(xlabel='Iteration', ylabel='Error')\n",
        "\n",
        "    def plot(self):\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        ax = plt.axes(projection='3d')\n",
        "\n",
        "        ax.scatter3D(self.X_train[:, 0], self.X_train[:, 1],\n",
        "                     self.sigmoid(self.X_train.dot(self.w)),\n",
        "                     c = self.y_train[:], cmap='viridis', s=100)\n",
        "\n",
        "        ax.set_xlim3d(55, 80)\n",
        "        ax.set_ylim3d(80, 240)\n",
        "        plt.ylabel('$x_2$ feature', fontsize=15)\n",
        "        plt.xlabel('$x_1$ feature', fontsize=15)\n",
        "        ax.set_zlabel('$P(Y = 1|x_1, x_2)$', fontsize=15, rotation = 0)\n",
        "\n",
        "    def scatterPlt(self):\n",
        "        x_min, x_max = 55, 80\n",
        "        y_min, y_max = 80, 240\n",
        "\n",
        "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 250),\n",
        "                             np.linspace(y_min, y_max, 250))\n",
        "        grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "        probs = grid.dot(self.w).reshape(xx.shape)\n",
        "\n",
        "        f, ax = plt.subplots(figsize=(14,12))\n",
        "\n",
        "        ax.contour(xx, yy, probs, levels=[0.5], cmap=\"Greys\", vmin=0, vmax=.6)\n",
        "\n",
        "        ax.scatter(self.X_train[:, 0], self.X_train[:, 1],\n",
        "                   c=self.y_train[:], s=50,\n",
        "                   cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
        "                   edgecolor=\"white\", linewidth=1)\n",
        "\n",
        "        plt.xlabel('x1 feature')\n",
        "        plt.ylabel('x2 feature')\n",
        "\n",
        "    def plot3D(self):\n",
        "        x_min, x_max = 55, 80\n",
        "        y_min, y_max = 80, 240\n",
        "\n",
        "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 250),\n",
        "                             np.linspace(y_min, y_max, 250))\n",
        "\n",
        "        grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "        probs = grid.dot(self.w).reshape(xx.shape)\n",
        "        fig = plt.figure(figsize=(14,12))\n",
        "        ax = plt.axes(projection='3d')\n",
        "        ax.contour3D(xx, yy, probs, 50, cmap='binary')\n",
        "\n",
        "        ax.scatter3D(self.X_train[:, 0], self.X_train[:, 1],\n",
        "                   c=self.y_train[:], s=50,\n",
        "                   cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
        "                   edgecolor=\"white\", linewidth=1)\n",
        "\n",
        "        ax.set_xlabel('x1')\n",
        "        ax.set_ylabel('x2')\n",
        "        ax.set_zlabel('probs')\n",
        "        ax.set_title('3D contour')\n",
        "        plt.show()\n",
        "\n",
        "    def predict(self, X):\n",
        "        sigmoid_value = self.sigmoid(X.dot(self.w))\n",
        "        return np.around(sigmoid_value)\n",
        "\n",
        "    def evaluate(self, y, y_hat):\n",
        "        y = (y == 1)\n",
        "        y_hat = (y_hat == 1)\n",
        "        accuracy = (y == y_hat).sum()/y.size\n",
        "        precision = (y & y_hat).sum()/y_hat.sum()\n",
        "        recall = (y & y_hat).sum()/y.sum()\n",
        "        return accuracy, precision, recall\n",
        "\n",
        "    def runModel(self):\n",
        "\n",
        "        self.w = np.ones(self.X_train.shape[1], dtype = np.float64) * 0\n",
        "        self.gradientDescent(self.X_train, self.y_train)\n",
        "        y_hat_train = self.predict(self.X_train)\n",
        "        accuracy, precision, recall = self.evaluate(self.y_train, y_hat_train)\n",
        "\n",
        "        print('\\nAccuracy :', round(accuracy,3)*100)\n",
        "        print('Precision :', round(precision,3))\n",
        "        print('Recall :', round(recall,3))"
      ],
      "metadata": {
        "id": "g0K1taTIaahd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an instance\n",
        "from tqdm import tqdm\n",
        "start = timeit.default_timer()\n",
        "lr = LogisticRegression(trainx, y_train_encoded,x_test, y_test_encoded,tolerance=0.0001, learningRate=0.1e-5, maxIteration=10000)\n",
        "lr.runModel()\n",
        "stop = timeit.default_timer()\n",
        "print('Time: ', round((stop - start),2), 'seconds')"
      ],
      "metadata": {
        "id": "BfzdbNW0E8ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks"
      ],
      "metadata": {
        "id": "OaK9gDAtdjnH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93d228c5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(28, input_dim=28, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(6, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "D5dT8eYwuFTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e99159c"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.00001),\n",
        "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "19ad6885"
      },
      "outputs": [],
      "source": [
        "hist=model.fit(\n",
        "    trainx,\n",
        "    y_train_encoded,\n",
        "    epochs=200,\n",
        "    validation_split=0.15, verbose=1, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8304835"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(x_test)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba832582"
      },
      "outputs": [],
      "source": [
        "lst=[]\n",
        "for i in range(0,len(y_pred)):\n",
        "     k=np.argmax(y_pred[i]) #it gives index value of the highest probability for each iteration\n",
        "     lst.append(k)\n",
        "y_pred_label=np.array(lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6a356c5"
      },
      "outputs": [],
      "source": [
        "matr = confusion_matrix(y_test_encoded, y_pred_label)\n",
        "sns.heatmap(matr, square=True, annot=True, cbar=False)\n",
        "plt.xlabel('Predicted value')\n",
        "plt.ylabel('True value');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f83f5d93"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_pred_label = np.argmax(y_pred, axis=1)\n",
        "# Now calculate the accuracy and recall\n",
        "print('Accuracy: %.3f' % accuracy_score(y_true=y_test_encoded, y_pred=y_pred_label))\n",
        "print('Recall (macro average): %.3f' % recall_score(y_true=y_test_encoded, y_pred=y_pred_label, average='macro'))\n",
        "print('Recall (weighted average): %.3f' % recall_score(y_true=y_test_encoded, y_pred=y_pred_label, average='weighted'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0058d0d3"
      },
      "outputs": [],
      "source": [
        "loss=hist.history['loss']\n",
        "def plot(loss):\n",
        "        axis=list(range(0, len(loss),1))\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.plot(axis, loss)\n",
        "        ax.set_xlabel('epoch')\n",
        "        ax.set_ylabel('loss')\n",
        "        ax.grid()\n",
        "        plt.show()\n",
        "plot(loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XuxetptInT2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Network V2"
      ],
      "metadata": {
        "id": "IUWVaOhUn00b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomNeuralNetwork:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, n_iters=100):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights_input_hidden = np.random.rand(self.input_size, self.hidden_size)\n",
        "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
        "        self.weights_hidden_output = np.random.rand(self.hidden_size, self.output_size)\n",
        "        self.bias_output = np.zeros((1, self.output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Input to hidden layer\n",
        "        self.hidden_layer_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_layer_output = self.sigmoid(self.hidden_layer_input)\n",
        "\n",
        "        # Hidden to output layer\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.output_layer_output = self.sigmoid(self.output_layer_input)\n",
        "\n",
        "        return self.output_layer_output\n",
        "\n",
        "    def train(self, X, y):\n",
        "        for i in range(self.n_iters):\n",
        "            # Forward pass\n",
        "            output = self.forward(X)\n",
        "\n",
        "            # Convert y to one-hot encoding\n",
        "            y_int = y.astype(int)  # Convert to integers\n",
        "            y_one_hot = np.eye(self.output_size)[y_int]\n",
        "\n",
        "            # Backward pass\n",
        "            self.backward(X, y_one_hot, output)\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        # Calculate output layer error\n",
        "        output_error = output - y\n",
        "        output_delta = output_error * self.sigmoid_derivative(output)\n",
        "\n",
        "        # Calculate hidden layer error\n",
        "        hidden_layer_error = output_delta.dot(self.weights_hidden_output.T)\n",
        "        hidden_layer_delta = hidden_layer_error * self.sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights_hidden_output += self.learning_rate * self.hidden_layer_output.T.dot(output_delta)\n",
        "        self.bias_output += self.learning_rate * np.sum(output_delta, axis=0, keepdims=True)\n",
        "        self.weights_input_hidden += self.learning_rate * X.T.dot(hidden_layer_delta)\n",
        "        self.bias_hidden += self.learning_rate * np.sum(hidden_layer_delta, axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "    def evaluate(self, y, y_hat):\n",
        "        acc_value = np.mean(y == y_hat)\n",
        "        prec_value = np.sum((y == 1) & (y_hat == 1)) / np.sum(y_hat == 1) if np.sum(y_hat == 1) != 0 else 0\n",
        "        rec_value = np.sum((y == 1) & (y_hat == 1)) / np.sum(y == 1) if np.sum(y == 1) != 0 else 0\n",
        "        return acc_value, prec_value, rec_value\n",
        "\n",
        "    def predict(self, X, Y):\n",
        "        output = self.forward(X)\n",
        "        predictions = np.round(output)\n",
        "        acc_value, prec_value, rec_value = self.evaluate(Y, predictions)\n",
        "        print(\"The Accuracy value is:\", np.round(acc_value, 2) * 100, \"%\")\n",
        "        print(\"The Precision value is:\", np.round(prec_value, 2))\n",
        "        print(\"The Recall value is:\", np.round(rec_value, 2))\n"
      ],
      "metadata": {
        "id": "tYiNEzt-dnp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = CustomNeuralNetwork(input_size=28, hidden_size=4, output_size=6, learning_rate=0.1, n_iters=10000)\n",
        "nn.train(trainx, y_train_encoded)"
      ],
      "metadata": {
        "id": "XHasiDltvrCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse=True)\n",
        "y_test_onehot = encoder.fit_transform(np.array(y_train_encoded).reshape(-1, 1))"
      ],
      "metadata": {
        "id": "d76AHpRs8QrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'nn.predict(x_test, y_test_onehot)'"
      ],
      "metadata": {
        "id": "NptZJgVe5FW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We attempted to build a neural network using equations and implemented the program. However, we encountered issues, especially when predicting with the model. To address this, we turned to standard libraries for a more reliable solution."
      ],
      "metadata": {
        "id": "iR59AT2DMIff"
      }
    }
  ]
}